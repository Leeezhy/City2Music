<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8" />
  <title>City Sound to Music</title>

  <!-- TensorFlow.js Âíå YAMNet Ê®°Âûã -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.9.0"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/yamnet@1.0.2/dist/yamnet.min.js"></script>

  <!-- Tone.js Èü≥È¢ëÂ∫ì -->
  <script src="https://cdn.jsdelivr.net/npm/tone@14.8.39/build/Tone.js"></script>

  <style>
    body {
      font-family: sans-serif;
      background-color: #f4f4f4;
      padding: 20px;
    }
    h1 {
      font-size: 24px;
    }
    button {
      font-size: 16px;
      padding: 10px 20px;
      margin-top: 10px;
    }
    #status {
      margin-top: 20px;
      font-size: 16px;
      color: green;
    }
  </style>
</head>
<body>
  <h1>üéß City Sound to Music</h1>
  <button id="startButton">Start Listening</button>
  <div id="status">Waiting to start...</div>

  <script>
    let model;
    let audioContext;
    let mic;
    let recognizerRunning = false;
    let labelNames = [];

    // ‚úÖ ‰ΩøÁî®ÂèØÁî®ÁöÑÈü≥È¢ëÈìæÊé•ÔºàÊù•Ëá™ tonejs-instruments È°πÁõÆÔºâ
    const sampler = new Tone.Sampler({
      urls: {
        C4: "https://cdn.jsdelivr.net/gh/nbrosowsky/tonejs-instruments/samples/piano/C4.mp3",
        D4: "https://cdn.jsdelivr.net/gh/nbrosowsky/tonejs-instruments/samples/piano/D4.mp3",
        E4: "https://cdn.jsdelivr.net/gh/nbrosowsky/tonejs-instruments/samples/piano/E4.mp3",
      },
      release: 1,
    }).toDestination();

    async function getTopLabels(scoresTensor, topK = 3) {
      const scores = (await scoresTensor.array())[0];
      const topIndices = scores
        .map((score, idx) => ({ score, idx }))
        .sort((a, b) => b.score - a.score)
        .slice(0, topK);
      return topIndices.map(obj => labelNames[obj.idx]);
    }

    document.getElementById("startButton").onclick = async () => {
      try {
        await Tone.start();
        if (recognizerRunning) return;
        recognizerRunning = true;

        document.getElementById("status").innerText = "Loading model...";
        model = await yamnet.load();
        labelNames = model.labels;
        document.getElementById("status").innerText = "Model loaded. Listening...";

        audioContext = new (window.AudioContext || window.webkitAudioContext)();
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        const mic = audioContext.createMediaStreamSource(stream);
        const processor = audioContext.createScriptProcessor(4096, 1, 1);
        mic.connect(processor);
        processor.connect(audioContext.destination);

        const buffer = [];

        processor.onaudioprocess = async (e) => {
          const input = e.inputBuffer.getChannelData(0);
          buffer.push(...input);
          if (buffer.length >= 16000) {
            const slice = buffer.splice(0, 16000);
            const inputTensor = tf.tensor(slice, [1, slice.length]);
            const topLabels = await getTopLabels(await model.predict(inputTensor));
            document.getElementById("status").innerText = "Detected: " + topLabels.join(", ");

            if (topLabels.includes("Car horn")) sampler.triggerAttackRelease("C4", "8n");
            if (topLabels.includes("Speech")) sampler.triggerAttackRelease("D4", "8n");
            if (topLabels.includes("Traffic noise")) sampler.triggerAttackRelease("E4", "8n");
          }
        };
      } catch (err) {
        console.error(err);
        document.getElementById("status").innerText = "‚ùå Error: " + err.message;
      }
    };
  </script>
</body>
</html>
